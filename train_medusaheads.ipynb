{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/takuro.fujii/.pyenv/versions/3.11.0/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Using `is_flash_attn_available` is deprecated and will be removed in v4.38. Please use `is_flash_attn_2_available` instead.\n",
      "Using `is_flash_attn_available` is deprecated and will be removed in v4.38. Please use `is_flash_attn_2_available` instead.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "import torch\n",
    "import numpy as np\n",
    "from medusa.model.medusa_model import MedusaModel\n",
    "from medusa.model.kv_cache import *\n",
    "from medusa.model.utils import *\n",
    "from medusa.model.medusa_choices import *\n",
    "from copy import deepcopy\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  3.47s/it]\n",
      "Some weights of MedusaModelLlama were not initialized from the model checkpoint at lmsys/vicuna-7b-v1.3 and are newly initialized: ['medusa_head.2.1.weight', 'medusa_head.4.1.weight', 'medusa_head.2.0.linear.bias', 'medusa_head.4.0.linear.weight', 'medusa_head.0.0.linear.weight', 'medusa_head.0.0.linear.bias', 'medusa_head.1.1.weight', 'medusa_head.3.1.weight', 'medusa_head.4.0.linear.bias', 'medusa_head.2.0.linear.weight', 'medusa_head.3.0.linear.bias', 'medusa_head.1.0.linear.bias', 'medusa_head.3.0.linear.weight', 'medusa_head.0.1.weight', 'medusa_head.1.0.linear.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# config\n",
    "model_name = 'FasterDecoding/medusa-vicuna-7b-v1.3'\n",
    "load_in_8bit = True\n",
    "load_in_4bit = False\n",
    "\n",
    "# モデル、トークナイザ\n",
    "model = MedusaModel.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float16,\n",
    "    low_cpu_mem_usage=True,\n",
    "    device_map=\"auto\",\n",
    "    load_in_8bit=load_in_8bit,\n",
    "    load_in_4bit=load_in_4bit,\n",
    ")\n",
    "tokenizer = model.get_tokenizer()\n",
    "\n",
    "medusa_choices = mc_sim_7b_63"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KVキャッシュの初期化\n",
    "past_key_values, past_key_values_data, current_length_data = initialize_past_key_values(model.base_model)\n",
    "model.past_key_values = past_key_values\n",
    "model.past_key_values_data = past_key_values_data\n",
    "model.current_length_data = current_length_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: I stay with my dog named Retrieva. Do you have any dogs?\n",
      "Input token length: 16\n"
     ]
    }
   ],
   "source": [
    "# プロンプトとトークナイズ確認\n",
    "model.current_length_data.zero_() # \n",
    "prompt = \"I stay with my dog named Retrieva. Do you have any dogs?\"\n",
    "input_ids = tokenizer([prompt]).input_ids\n",
    "input_len = len(input_ids[0])\n",
    "\n",
    "print(\"Prompt:\", prompt)\n",
    "print('Input token length:', len(input_ids[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output shape: torch.Size([1, 16, 32000])\n",
      "First prediction: ['itel', \"'\", 'in', 'my', 'parents', ',', 'Max', 'ver', '.', 'She', 'you', 'have', 'any', 'p', '?', '\\n']\n"
     ]
    }
   ],
   "source": [
    "with torch.inference_mode():\n",
    "    model.current_length_data.zero_()\n",
    "    output = model.base_model(torch.as_tensor(input_ids).cuda(), past_key_values=model.past_key_values,)\n",
    "    print('output shape:', output.logits.shape)\n",
    "    pred = output.logits.argmax(-1)\n",
    "    input_ids[0] = input_ids[0] + pred[0, -1:].tolist()\n",
    "    print('First prediction:', tokenizer.batch_decode(pred[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:10<00:00,  9.29it/s]\n"
     ]
    }
   ],
   "source": [
    "with torch.inference_mode():\n",
    "    for _ in tqdm(range(100)):\n",
    "        output = model.base_model(pred[..., -1:], past_key_values=model.past_key_values, use_cache=True)\n",
    "        pred = output.logits.argmax(-1)\n",
    "        # pred_topk = output.logits.topk(10, dim = -1).indices[0]\n",
    "        input_ids[0] = input_ids[0] + pred[0, -1:].tolist()\n",
    "        #print(tokenizer.batch_decode(pred[..., -1:]))\n",
    "        # print(tokenizer.batch_decode(pred_topk), 'topk:', pred_topk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> I stay with my dog named Retrieva. Do you have any dogs?\n",
      "Yes, I have a dog named Max. He'<s> The 2019-2024 Outlook for Non-Alcoholic Beverages in India\n",
      "This study covers the latent demand outlook for non-alcoholic beverages across the states, union territories, and cities of India. Latent demand (in millions of U.S. dollars) or the number of units of non-alcoholic beverages is estimated\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(input_ids[0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
